{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝10_앙상블학습_랜덤포레스트_GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coding: utf-8\n",
    "\n",
    "# 앙상블 학습\n",
    "\n",
    "## 앙상블 학습 (Ensemble Learning) 개요\n",
    "\n",
    "### 앙상블 학습을 통한 분류\n",
    "- 여러 개의 분류기(Classifier)을 사용해서 예측 결합함으로써 보다 정확한 최종 예측을 도출하는 기법\n",
    "- 단일 분류기 사용 때보다 신뢰성이 높은 예측값을 얻을 수 있음\n",
    "- 쉽고 편하면서도 강력한 성능 보유\n",
    "- 대부분의 정형 데이터 분류 시 뛰어난 성능을 나타냄\n",
    "- 이미지, 영상, 음성 등의 비정형 데이터 분류 : 딥러닝 성능 뛰어남\n",
    "\n",
    "### 대표적인 앙상블 알고리즘\n",
    "- 랜덤 포레스트(Random Forrest)\n",
    "- 그레디언트 부스팅(Gradient Boosting)\n",
    "\n",
    "### 앙상블 알고리즘 변화\n",
    "- 뛰어난 성능, 쉬운 사용, 다양한 활용도로 인해 많이 애용되었고\n",
    "- 부스팅 계열의 앙상블 알고리즘의 인기와 강세가 계속 이어져\n",
    "- 기존의 그레디언트 부스팅을 뛰어넘는 새로운 알고리즘 가속화\n",
    "\n",
    "**최신 앙상블 알고리즘**\n",
    "- XGBoost\n",
    "- LightBGM : XGBoost와 예측 성능 유사하면서도 수행 속도 훨씬 빠름\n",
    "- Stacking : 여러 가지 모델의 결과를 기반으로 메타 모델 수립\n",
    "\n",
    "\n",
    "XGBoost, LightBGM과 같은 최신 앙상블 알고리즘 한두 개만 잘 알고 있어도\n",
    "정형 데이터의 분류 또는 회귀 분야에서 예측 성능이 매우 뛰어난 모델을 쉽게 만들 수 있음\n",
    "\n",
    "## 앙상블 학습 유형\n",
    "- 보팅(Voting)\n",
    "- 배깅(Bagging)\n",
    "- 부스팅(Boosting)\n",
    "- 스태킹(Stacking)\n",
    "\n",
    "보팅(Voting) : 여러 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식\n",
    "- 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합\n",
    "\n",
    "배깅(Bagging) : 보팅과 동일하게 여러 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식\n",
    "- 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만, \n",
    "- 샘플링을 서로 다르게 하면서 학습 수행\n",
    "- 대표적인 배깅 방식 : 랜덤 포레스트 알고리즘\n",
    "\n",
    "\n",
    "보팅 분류기 도식화\n",
    "- 선형회귀, K최근접 이웃, 서포트 벡터 머신 3개의 ML 알고리즘이\n",
    "- 같은 데이터 세트에 대해 학습하고 예측한 결과를 가지고\n",
    "- 보팅을 통해 최종 예측 결과를 선정\n",
    "\n",
    "배깅 분류기 도식화\n",
    "- 단일 ML 알고리즘(결정트리)만 사용해서\n",
    "- 여러 분류기가 각각의 샘플링된 데이터 세트에 대해 학습하고 개별 예측한 결과를\n",
    "- 보팅을 통해 최종 예측 결과 선정\n",
    "\n",
    "샘플링 방식 : 부트 스트래핑 분할 방식\n",
    "- 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식\n",
    "- 각 샘플링된 데이터 내에는 중복 데이터 포함\n",
    "- (교차 검증에서는 데이터 세트 간에 중첩 허용하지 않음)\n",
    "\n",
    "https://swalloow.github.io/bagging-boosting/\n",
    "\n",
    "### 부스팅(Boosting)\n",
    "- 여러 개의 분류기가 순차적으로 학습 수행하되\n",
    "- 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록\n",
    "- 다음 분류기에게는 가중치(weight)를 부여하면서 \n",
    "- 학습과 예측을 진행하는 방식\n",
    "- 예측 성능이 뛰어나 앙상블 학습 주도\n",
    "- boost : 밀어 올림\n",
    "    \n",
    "**대표적인 부스팅 모듈**\n",
    "- Gradient Boost\n",
    "- XGBost(eXtra Gradient Boost)\n",
    "- LightGBM(Light Gradient Boost)\n",
    "\n",
    "### 스태킹\n",
    "- 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어로\n",
    "- 다른 모델(메타 모델)로 재학습시켜 결과를 예측하는 방식\n",
    "\n",
    "### 보팅 유형 \n",
    "- 하드 보팅 (Hard Voting)\n",
    "- 소프트 보팅 (Soft Voting)\n",
    "\n",
    "하드 보팅 (Hard Voting)\n",
    "- 다수결 원칙과 유사\n",
    "- 예측한 결과값들 중에서 \n",
    "- 다수의 분류기가 결정한 예측값을\n",
    "- 최종 보팅 결과값으로 선정\n",
    "\n",
    "소프트 보팅 (Soft Voting)\n",
    "- 분류기들의 레이블 값 결정 확률을 평균내서\n",
    "- 확률이 가장 높은 레이블 값을\n",
    "- 최종 보팅 결과값으로 선정\n",
    "- 일반적으로 소프트 보팅이 예측 성능이 좋아서 더 많이 사용\n",
    "\n",
    "\n",
    "하드 보팅 도식화\n",
    "- Classifier 1, 2, 3, 4번 4개로 구성\n",
    "- 분류기 1, 3, 4번 예측 : 레이블 값 1로 예측\n",
    "- 분류기 2번 예측 : 2로 예측\n",
    "- 다수결 원칙에 따라서 최종 예측은 레이블 값 1\n",
    "\n",
    "소프트 보팅  도식화\n",
    "- 레이블 값1과 레이블 값2에 대한 분류기 별 예측 확률\n",
    "- 1번 : 0.7, 0.3\n",
    "- 2번 : 0.2, 0.8\n",
    "- 3번 : 0.8, 0.2\n",
    "- 4번 : 0.5, 0.1\n",
    "- 레이블 값 1예 대한 예측 확률 평균 : 0.64 \n",
    "- 레이블 값 2예 대한 예측 확률 평균 : 0.35 \n",
    "- 최종 레이블 값 1로 최종 보팅\n",
    "\n",
    "## Voting Classifier\n",
    "\n",
    "### 보팅 방식의 앙상블 예제 : 위스콘신 유방암 데이터 세트 예측 분석  \n",
    "        \n",
    "**위스콘신 유방암 데이터 세트**\n",
    "- 유방암의 악성종양, 양성종양 여부를 결정하는 이진 분류 데이터 세트\n",
    "- 종양의 크기, 모양 등의 형태와 관련한 많은 피처 포함\n",
    "- 사이킷런의 보팅 양식의 앙상블을 구현한 VotingClassifier 클래스를 이용해서 보팅 분류기 생성  \n",
    "- `load_breast_cancer()` 함수를 통해 위스콘신 유방암 데이터 세트 생성\n",
    "- 로지스틱 회귀와 KNN 기반으로 소프트 보팅 방식으로 보팅 분류기 생성\n",
    "\n",
    "### 위스콘신 유방암 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# 피처 확인\n",
    "data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.data.shape\n",
    "\n",
    "\n",
    "# **VotingClassifier로 개별모델은 로지스틱 회귀와 KNN을 보팅방식으로 결합하고 성능 비교**\n",
    "\n",
    "# VotingClassifier 클래스의 주요 생성 인자\n",
    "# - estimators : 리스트 값으로 보팅에 사용될 여러 개의 Classifier 객체들을 튜플 형식으로 입력 받음\n",
    "# - voting : 보팅 방식 - hard/soft (디폴트 : hard) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 모델은 로지스틱 회귀와 KNN 임. \n",
    "lr_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "# 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 \n",
    "\n",
    "# estimators 복수 : 리스트형태\n",
    "# lr_clf 이름을 'LR'로\n",
    "# knn_clf 이름을 'KNN'으로 \n",
    "# 보팅 방식 : 디폴트 hard\n",
    "\n",
    "vo_clf = VotingClassifier(estimators=[('LR', lr_clf),('KNN', knn_clf)], voting='soft')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    test_size=0.2, random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보팅을 통한 분류기 정확도: 0.9474\n"
     ]
    }
   ],
   "source": [
    "# VotingClassifier 학습/예측/평가. \n",
    "# 개별 모델들이 다 학습하고 예측한 결과로 평가\n",
    "vo_clf.fit(X_train, y_train)\n",
    "pred = vo_clf.predict(X_test)\n",
    "print('보팅을 통한 분류기 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 정확도: 0.9386\n",
      "KNeighborsClassifier 정확도: 0.9386\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀와 KNN 각개별 모델의 학습/예측/평가.\n",
    "classifiers = [lr_clf, knn_clf]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 보팅 분류기가 정확도가 조금 높게 나타났는데\n",
    "보팅으로 여러 개의 기반 분류기를 결합한다고 해서 무조건 예측 성능이 더 향상되지는 않음\n",
    "- 보팅, 배깅, 부스팅 등의 앙상블 방법은\n",
    "전반적으로 다른 단일 ML 알고리즘 보다 뛰어난 예측 성능을 가지는 경우가 많음\n",
    "\n",
    "- 고정된 데이터 세트에서 단일 ML 알고리즘이 뛰어난 성능을 발휘하더라도\n",
    "현실 세계는 다양한 변수와 예측이 어려운 규칙으로 구성되어있기 때문에\n",
    "다양한 관점을 가진 알고리즘이 서로 결합해 더 나은 성능을 실제 환경에서 끌어낼 수 있음\n",
    "    \n",
    "저자 설명    \n",
    "- 살짝 만들어진 결과 냄새가 난다. 그런쪽으로 좀 유도도 하긴 했음\n",
    "- 모든 걸 다 합친다고 개별 모델보다 무조건 좋아지라는 법은 없음\n",
    "- 똑똑한 하나가 여러 개를 합친 것 보다 좋을 수도 있고, 그렇지 않을 수도 있고\n",
    "- 보팅을 통해서 개별 모델들을 합치면 좋아질 수 있는 가능성이 있다 정도로 이해하면 됨\n",
    "\n",
    "# 랜덤 포레스트(Random Forest)\n",
    "\n",
    "### 배깅(Bagging)\n",
    "- 보팅과는 다르게 같은 알고리즘으로 여러 개의 분류기를 만들어서 보팅으로 최종 결정하는 알고리즘\n",
    "- 대표적 배깅 알고리즘 : 랜덤 포레스트\n",
    "    \n",
    "### 랜덤 포레스트(Random Forest)\n",
    "- 다재 다능한 알고리즘\n",
    "- 앙상블 알고리즘 중 수행 속도가 빠르고\n",
    "- 다양한 영역에서 높은 예측 성능을 보임\n",
    "- 기반 알고리즘은 결정 트리\n",
    "- 결정 트리의 쉽고 직관적인 장점을 그대로 채택\n",
    "- (대부분의 부스팅 기반의 다양한 알고리즘 역시 \n",
    "- 결정 트리 알고리즘을 기반 알고리즘으로 채택)\n",
    "\n",
    "### 랜덤 포레스트의 예측 결정 방식\n",
    "- 여러 개의 결정 트리 분류기가\n",
    "- 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링하여\n",
    "- 개별적으로 학습을 수행한 뒤\n",
    "- 최정적으로 모든 분류기가 보팅을 통해 예측 결정\n",
    "\n",
    "\n",
    "### 랜덤 포레스트에서의 부트스트래핑 샘플링 방식\n",
    "\n",
    "**부트스트래핑(bootstrapping) 분할 방식**\n",
    "- 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식\n",
    "- 각 샘플링된 데이터 내에는 중복 데이터 포함\n",
    "\n",
    "**랜덤 포레스트 부트 부트 스트래핑 분할**\n",
    "- 개별적인 분류기의 기반 알고리즘은 결정 트리\n",
    "- 개별 트리가 학습하는 데이터 세트는 전체 데이터에서 일부가 중복되게 샘플링된 데이터 세트\n",
    "- Subset 데이터는 이러한 부트 스트래핑으로 데이터가 임의로 만들어짐\n",
    "- Subset 데이터 건수는 전체 데이터 건수와 동일하지만 개별 데이터가 중복되어 만들어짐\n",
    "\n",
    "\n",
    "- 예 : 원본 데이터 건수가 10개인 학습 데이터 세트\n",
    "    - 랜덤 포레스트를 3개의 결정 트리 기반으로 학습하려고\n",
    "    - n_estimators = 3으로 하이퍼 파라미터를 부여한 경우\n",
    "\n",
    "\n",
    "## 랜덤 포레스트 예제\n",
    "- 앞의 사용자 행동 인식 데이터 세트를 \n",
    "- 사이킷런의 RandomForestClassifier 클래스를 이용해 예측 수행\n",
    "\n",
    "### 결정 트리에서 사용한 사용자 행동 인지 데이터 세트 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞에서 결정트리 예제에서 작성했음 (복사해서 사용)\n",
    "\n",
    "# 피처명 변경해서 반환하는 과정을 함수로 작성\n",
    "# 피처명_1 또는 피처명_2로 변경\n",
    "# groupby('column_name').cumcount() : 중복되는 값이 몇 번째에 해당되는지(index) 반환\n",
    "# 0이면 첫 번째, 1이면 두 번째, ...\n",
    "\n",
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
    "                                                                                           if x[1] >0 else x[0], axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/human_activity/features.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b46f74e220ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_human_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-b46f74e220ba>\u001b[0m in \u001b[0;36mget_human_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     feature_name_df = pd.read_csv('data/human_activity/features.txt',sep='\\s+',\n\u001b[1;32m----> 9\u001b[1;33m                         header=None,names=['column_index','column_name'])\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m         )\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/human_activity/features.txt'"
     ]
    }
   ],
   "source": [
    "# 앞에서 작성했음 (복사해서 사용)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_human_dataset( ):\n",
    "    \n",
    "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\n",
    "    feature_name_df = pd.read_csv('data/human_activity/features.txt',sep='\\s+',\n",
    "                        header=None,names=['column_index','column_name'])\n",
    "    \n",
    "    # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. \n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "    \n",
    "    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "    \n",
    "    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv('data/human_activity/train/X_train.txt',sep='\\s+', names=feature_name )\n",
    "    X_test = pd.read_csv('data/human_activity/test/X_test.txt',sep='\\s+', names=feature_name)\n",
    "    \n",
    "    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여\n",
    "    y_train = pd.read_csv('data/human_activity/train/y_train.txt',sep='\\s+',header=None,names=['action'])\n",
    "    y_test = pd.read_csv('data/human_activity/test/y_test.txt',sep='\\s+',header=None,names=['action'])\n",
    "    \n",
    "    # 로드된 학습/테스트용 DataFrame을 모두 반환 \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습/테스트 데이터로 분리하고 랜덤 포레스트로 학습/예측/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/human_activity/features.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-985baaaf2b48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_human_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-b46f74e220ba>\u001b[0m in \u001b[0;36mget_human_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     feature_name_df = pd.read_csv('data/human_activity/features.txt',sep='\\s+',\n\u001b[1;32m----> 9\u001b[1;33m                         header=None,names=['column_index','column_name'])\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m         )\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\multi\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/human_activity/features.txt'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "rf_clf.fit(X_train , y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test , pred)\n",
    "print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy))\n",
    "# 수행속도 괜찮게 나왔지만 좋은 편은 아니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트리 기반의 앙상블 알고리즘의 단점\n",
    "- 하이퍼파라미터가 너무 많고\n",
    "- 그로 인해서 튜닝을 위한 시간이 많이 소모된다는 것\n",
    "- 또한 많은 시간을 소모했으나 튜닝 후 예측 성능이 크게 향상되는 경우가 많지 않음\n",
    "- 트리 기반 자체의 하이퍼파라미터가 원래 많으며\n",
    "    - 배깅, 부스팅, 학습, 정규화 등을 위한 하이퍼 파라미터까지 추가되므로\n",
    "    - 일반적으로 다른 ML 알고리즘에 비해 많을 수 밖에 없음\n",
    "- 그나마 랜덤 포레스트가 적은 편에 속하는데,\n",
    "- 결정 트리에서 사용되는 하이퍼 파라미터와 같은 파라미터가 대부분이기 때문\n",
    "\n",
    "### GridSearchCV 로 교차검증 및 하이퍼 파라미터 튜닝\n",
    "\n",
    "**GridSearchCV 로 교차검증 및 하이퍼 파라미터 튜닝**\n",
    "- 앞의 사용자 행동 데이터 세트 그대로 사용\n",
    "- 튜닝 시간을 절약하기 위해 \n",
    "    - n_estimators=100\n",
    "    - cv=2\n",
    "    \n",
    "예제 수행 시간 오래 걸림\n",
    "- 멀티 코어 환경에서는 빠르게 학습이 가능\n",
    "- 그래서 그래디언트 부스팅보다 예측 성능이 약간 떨어지더라도\n",
    "- 랜덤 포레스트로 일단 기반 모델을 먼저 구축하는 경우가 많음\n",
    "- 멀티 코어 환경에서는 n_jobs=-1로 추가하면 모든 CPU 코어 이용해 학습\n",
    "\n",
    "n_estimators : 결정 트리의 개수. 디폴트 10\n",
    "- 많이 설정할수록 좋은 성능을 기대할 수 있지만\n",
    "- 계속 증가시킨다고 무조건 향샹되는 것은 아님\n",
    "- 또 증가시킬수록 학습 수행 시간이 오래 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터:\n",
      " {'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 100}\n",
      "최고 예측 정확도: 0.9451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 테스트해 볼 데이터를 많이 넣으면 기하급수적으로 늘어난다\n",
    "params = {\n",
    "    'n_estimators':[100], # 1차적으로 100으로 줄이고,나중에 최적화 되면 늘려서 최종적으로 예측 수행\n",
    "    'max_depth' : [6, 8, 10, 12], \n",
    "    'min_samples_leaf' : [8, 12, 18 ],\n",
    "    'min_samples_split' : [8, 16, 20]  # 4x3x3 : 36번\n",
    "}\n",
    "\n",
    "# RandomForestClassifier 객체 생성 후 GridSearchCV 수행\n",
    "rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1) \n",
    "\n",
    "# n_jobs=-1 : 전체 cpu 콜을 다 활용하라는 거고\n",
    "# n_jobs=-1 를 사용하면 개인 pc가 굉장히 많은 수행 성능을 잡아 먹기 때문에 느려짐\n",
    "# 수행하면 한 2분 걸림\n",
    "\n",
    "grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 ) \n",
    "# cv 2개 : 너무 많이 하면 실행시간이 오래걸리니까 (총 72번 수행 : 36 x 2)\n",
    "\n",
    "grid_cv.fit(X_train , y_train)\n",
    "\n",
    "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))\n",
    "\n",
    "# 결과\n",
    "# 출력된 최적 하이퍼 파라미터일 때 최고 예측 정확도: 91.8 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 튜닝된 하이퍼 파라미터로 재 학습 및 예측/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 0.9386\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV로 찾은 최적의 하이퍼 파라미터를\n",
    "# 랜덤 포레스트에 적용해서 예측 수행\n",
    "\n",
    "# 이번에는 n_estimators=300으로 늘림\n",
    "# 위 결과의 최적 하이퍼 파라미터들을 다 입력해서 \n",
    "# RandomForestClassifier 초기화시키고\n",
    "# 예측 성능 측정\n",
    "\n",
    "rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8,\n",
    "                                 min_samples_split=8, random_state=0)\n",
    "rf_clf1.fit(X_train , y_train)\n",
    "pred = rf_clf1.predict(X_test)\n",
    "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개별 feature들의 중요도 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-940b79ba45e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 앞으로 계속 중요도 시각화를 죽 계속할 건데 이 코드를 계속 비슷하게 사용한다고 생각하면 됨\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mftr_importances_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_clf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mftr_importances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftr_importances_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# sort_values() 쉽게 하기 위해서 시리즈로 만들고,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 최고 중요도가 높은 20개 피처들만 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# 결정 트리에서처럼 feature_importances_ 속성을 이용해서  \n",
    "# 알고리즘이 선택한 피처의 중요도를 알 수 있음  \n",
    "# 피처들의 중요도를 막대그래프로 시각화 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# 앞으로 계속 중요도 시각화를 죽 계속할 건데 이 코드를 계속 비슷하게 사용한다고 생각하면 됨\n",
    "ftr_importances_values = rf_clf1.feature_importances_\n",
    "ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns)\n",
    "# sort_values() 쉽게 하기 위해서 시리즈로 만들고, \n",
    "# 최고 중요도가 높은 20개 피처들만 추출\n",
    "ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature importances Top 20')\n",
    "# x축은 중요도 값, y축은 ftr_top20 시리즈의 index\n",
    "sns.barplot(x=ftr_top20 , y = ftr_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM(Gradient Boosting Machine)\n",
    "\n",
    "### 부스팅(Boosting)\n",
    "- 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서\n",
    "- 잘못 예측된 데이터에 가중치(weight) 부여를 통해\n",
    "- 오류를 개선해 나가면서 학습하는 방식\n",
    "\n",
    "### 대표적 부스팅 알고리즘\n",
    "- AdaBoost(Adaptive Boosting) : 에이다 부스트\n",
    "- GBM(Gradient Boosting Machine) : 그래디언트 부스트\n",
    "\n",
    "\n",
    "## GBM(Gradient Boosting Machine) : 그래디언트 부스트\n",
    "- 에이다 부스트와 유사하지만\n",
    "- 가중치 업데이터를 경사 하강법을 이용하는 것이 큰 차이\n",
    "    - 반복 수행을 통해 오류를 최소화할 수 있도록\n",
    "    - 가중치의 업데이트 값을 도출\n",
    "    - 오류값 = 실제값 - 예측값\n",
    "- 분류와 회귀 둘 다 가능\n",
    "\n",
    "경사 하강법(Gradient Descent)\n",
    "- 함수의 기울기(경사)를 구하고 경사의 절대값이 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것(위키백과)\n",
    "- 제시된 함수의 기울기로 최소값을 찾아내는 머신러닝 알고리즘\n",
    "- 매개변수를 반복적으로 조정해서 최소 함수값을 갖게하는 독립변수를 찾는 방법\n",
    "\n",
    "CART 기반 알고리즘\n",
    "- Classification And Regression Tree\n",
    "- 분류와 회귀 다 가능한 알고리즘\n",
    "\n",
    "## GBM 예제\n",
    "- GBM을 이용해 사용자 행동 데이터 세트를 예측 분류 수행\n",
    "- GBM 학습하는 시간이 얼마나 걸리는지 GBM 수행 시간 측정\n",
    "- 사이킷런의 GradientBoostingClassifier 클래스 사용\n",
    "- 앞에서 작성한 get_new_feature_name_df() 함수와 get_human_dataset( ) 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "# GBM 수행 시간 측정을 위함. 시작 시간 설정.\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train , y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "print(\"GBM 수행 시간: {0:.1f} 초 \".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "결과\n",
    "기본 하이퍼 파라미터만으로도 93.69 %의 예측 정확도로\n",
    "앞의 튜닝된 하이퍼 파라미터로 재 학습 및 예측/평가한\n",
    "랜덤 포레스트(91.65 %)보다 나은 예측 성능을 나타냄\n",
    "일반적으로 GBM이 랜덤 포레스트보다 예측 성능이 조금 뛰어난 경우가 많음\n",
    "문제 : 시간 오래 걸리고 하이퍼 파라미터 튜닝 노력도 더 필요\n",
    "\n",
    "GBM이 극복해야 할 중요 과제 : 수행 시간\n",
    "사이킷런의 GradientBoostingClassifier는 \n",
    "약한 학습기의 순차적인 예측 오류 부정을 통해 학습을 수행하므로\n",
    "멀티 CPU 코어 시스템을 사용하더라도 병렬 처리가 지원되지 않아서\n",
    "대용량 데이터의 경우 학습에 매우 많이 시간 필요\n",
    "\n",
    "데이터가 커지면 커질수록 너무 오래 걸려서\n",
    "하이퍼 파라미터 튜닝하기 많이 어려움\n",
    "\n",
    "반면에 랜덤 포레스트의 경우 상대적으로 빠른 수행시간을 보장해주기 때문에\n",
    "더 쉽게 예측 결과 도출 가능\n",
    "\n",
    "\n",
    " ## GBM 하이퍼 파라미터 및 튜닝\n",
    "\n",
    "### GBM의 주요 하이퍼 파라미터  \n",
    "\n",
    "**`loss`** : 경사 하강법에서 사용할 비용 함수 지정. 기본값은 'deviance'\n",
    "    \n",
    "**`n_estimators`** : weak learner의 개수. 기본값 100\n",
    "- weak learner가 순차적으로 오류를 보정하므로\n",
    "- 개수가 많을수록 예측 성능이 일정 수준까지 좋아질 수 있음\n",
    "- 그러나 개수가 많을 수록 시간이 오래 걸림\n",
    "\n",
    "**`learning_rate`** : GBM이 학습을 진행할 때마다 적용하는 학습률\n",
    "- weak learner가 순차적으로 오류값을 보정해 나가는 데 적용하는 계수\n",
    "- 0 ~ 1 사이의 값 지정 (기본값 0.1)\n",
    "- 작은 값을 적용하면 업데이트 되는 값이 작아져서\n",
    "    - 최소 오류 값을 찾아 예측 성능이 높아질 가능성은 높지만\n",
    "    - 많은 weak learner의 순차적인 반복 작업에 수행 시간이 올래 걸림\n",
    "- 너무 작게 설정하면 모든 weak learner의 반복이 완료되어도\n",
    "    - 최소 오류값을 찾지 못할 수도 있음\n",
    "- 반대로 큰 값을 적용하면 최소 오류값을 찾지 못하고 그냥 지차져 버려\n",
    "    - 예측 성능이 떨어질 가능성이 높아지지만 빠른 수행은 가능\n",
    "\n",
    "**`subsample`** : weak learner가 학습에 사용하는 데이터의 샘플링 비율\n",
    "- 기본값 1 : 전체 학습 데이터를 기반으로 학습한다는 의미\n",
    "- 0.5 : 학습 데이터의 50%\n",
    "- 과적합이 염려되는 경우 1보다 작은 값으로 설정\n",
    "\n",
    "### GridSearchCV 이용해서 하이퍼 파라미터 최적화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gb_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8a603bfb05b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;34m'learning_rate'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m }\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mgrid_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgb_clf\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'최적 하이퍼 파라미터:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gb_clf' is not defined"
     ]
    }
   ],
   "source": [
    "# GridSearchCV 이용해서 하이퍼 파라미터 최적화\n",
    "# 사용자 행동 데이터 세트 정도의 데이터 양에\n",
    "# 많은 하이퍼 파라미터로 튜닝하게 되면 시간이 상당히 오래 걸림\n",
    "# 간략하게 n_estimators와 learning_rate만 적용\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators':[100, 500],\n",
    "    'learning_rate' : [ 0.05, 0.1]\n",
    "}\n",
    "grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1)\n",
    "grid_cv.fit(X_train , y_train)\n",
    "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))\n",
    "\n",
    "# 한 30분 걸릴 것이다\n",
    "\n",
    "# 결과\n",
    "# learning_rate이 0.05, n_estimators가 500일 때\n",
    "# 2개의 교차 검증 세트에서 90.1 %의 최고 예측 정확도 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 6, 'min_samples_leaf': 8, 'min_s...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 6, 'min_samples_leaf': 8, 'min_s...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 6, 'min_samples_leaf': 8, 'min_s...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 6, 'min_samples_leaf': 12, 'min_...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 6, 'min_samples_leaf': 12, 'min_...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_depth': 6, 'min_samples_leaf': 12, 'min_...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'max_depth': 6, 'min_samples_leaf': 18, 'min_...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'max_depth': 6, 'min_samples_leaf': 18, 'min_...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'max_depth': 6, 'min_samples_leaf': 18, 'min_...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'max_depth': 8, 'min_samples_leaf': 8, 'min_s...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'max_depth': 8, 'min_samples_leaf': 8, 'min_s...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'max_depth': 8, 'min_samples_leaf': 8, 'min_s...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'max_depth': 8, 'min_samples_leaf': 12, 'min_...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'max_depth': 8, 'min_samples_leaf': 12, 'min_...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'max_depth': 8, 'min_samples_leaf': 12, 'min_...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'max_depth': 8, 'min_samples_leaf': 18, 'min_...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'max_depth': 8, 'min_samples_leaf': 18, 'min_...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'max_depth': 8, 'min_samples_leaf': 18, 'min_...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 8, 'min_...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 8, 'min_...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 8, 'min_...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 12, 'min...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 12, 'min...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 12, 'min...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 18, 'min...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 18, 'min...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_leaf': 18, 'min...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'max_depth': 12, 'min_samples_leaf': 8, 'min_...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'max_depth': 12, 'min_samples_leaf': 8, 'min_...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'max_depth': 12, 'min_samples_leaf': 8, 'min_...</td>\n",
       "      <td>0.945069</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'max_depth': 12, 'min_samples_leaf': 12, 'min...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'max_depth': 12, 'min_samples_leaf': 12, 'min...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'max_depth': 12, 'min_samples_leaf': 12, 'min...</td>\n",
       "      <td>0.938490</td>\n",
       "      <td>13</td>\n",
       "      <td>0.925439</td>\n",
       "      <td>0.951542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'max_depth': 12, 'min_samples_leaf': 18, 'min...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'max_depth': 12, 'min_samples_leaf': 18, 'min...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'max_depth': 12, 'min_samples_leaf': 18, 'min...</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>25</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.955947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_score  \\\n",
       "0   {'max_depth': 6, 'min_samples_leaf': 8, 'min_s...         0.945069   \n",
       "1   {'max_depth': 6, 'min_samples_leaf': 8, 'min_s...         0.945069   \n",
       "2   {'max_depth': 6, 'min_samples_leaf': 8, 'min_s...         0.945069   \n",
       "3   {'max_depth': 6, 'min_samples_leaf': 12, 'min_...         0.938490   \n",
       "4   {'max_depth': 6, 'min_samples_leaf': 12, 'min_...         0.938490   \n",
       "5   {'max_depth': 6, 'min_samples_leaf': 12, 'min_...         0.938490   \n",
       "6   {'max_depth': 6, 'min_samples_leaf': 18, 'min_...         0.934114   \n",
       "7   {'max_depth': 6, 'min_samples_leaf': 18, 'min_...         0.934114   \n",
       "8   {'max_depth': 6, 'min_samples_leaf': 18, 'min_...         0.934114   \n",
       "9   {'max_depth': 8, 'min_samples_leaf': 8, 'min_s...         0.945069   \n",
       "10  {'max_depth': 8, 'min_samples_leaf': 8, 'min_s...         0.945069   \n",
       "11  {'max_depth': 8, 'min_samples_leaf': 8, 'min_s...         0.945069   \n",
       "12  {'max_depth': 8, 'min_samples_leaf': 12, 'min_...         0.938490   \n",
       "13  {'max_depth': 8, 'min_samples_leaf': 12, 'min_...         0.938490   \n",
       "14  {'max_depth': 8, 'min_samples_leaf': 12, 'min_...         0.938490   \n",
       "15  {'max_depth': 8, 'min_samples_leaf': 18, 'min_...         0.934114   \n",
       "16  {'max_depth': 8, 'min_samples_leaf': 18, 'min_...         0.934114   \n",
       "17  {'max_depth': 8, 'min_samples_leaf': 18, 'min_...         0.934114   \n",
       "18  {'max_depth': 10, 'min_samples_leaf': 8, 'min_...         0.945069   \n",
       "19  {'max_depth': 10, 'min_samples_leaf': 8, 'min_...         0.945069   \n",
       "20  {'max_depth': 10, 'min_samples_leaf': 8, 'min_...         0.945069   \n",
       "21  {'max_depth': 10, 'min_samples_leaf': 12, 'min...         0.938490   \n",
       "22  {'max_depth': 10, 'min_samples_leaf': 12, 'min...         0.938490   \n",
       "23  {'max_depth': 10, 'min_samples_leaf': 12, 'min...         0.938490   \n",
       "24  {'max_depth': 10, 'min_samples_leaf': 18, 'min...         0.934114   \n",
       "25  {'max_depth': 10, 'min_samples_leaf': 18, 'min...         0.934114   \n",
       "26  {'max_depth': 10, 'min_samples_leaf': 18, 'min...         0.934114   \n",
       "27  {'max_depth': 12, 'min_samples_leaf': 8, 'min_...         0.945069   \n",
       "28  {'max_depth': 12, 'min_samples_leaf': 8, 'min_...         0.945069   \n",
       "29  {'max_depth': 12, 'min_samples_leaf': 8, 'min_...         0.945069   \n",
       "30  {'max_depth': 12, 'min_samples_leaf': 12, 'min...         0.938490   \n",
       "31  {'max_depth': 12, 'min_samples_leaf': 12, 'min...         0.938490   \n",
       "32  {'max_depth': 12, 'min_samples_leaf': 12, 'min...         0.938490   \n",
       "33  {'max_depth': 12, 'min_samples_leaf': 18, 'min...         0.934114   \n",
       "34  {'max_depth': 12, 'min_samples_leaf': 18, 'min...         0.934114   \n",
       "35  {'max_depth': 12, 'min_samples_leaf': 18, 'min...         0.934114   \n",
       "\n",
       "    rank_test_score  split0_test_score  split1_test_score  \n",
       "0                 1           0.938596           0.951542  \n",
       "1                 1           0.938596           0.951542  \n",
       "2                 1           0.938596           0.951542  \n",
       "3                13           0.925439           0.951542  \n",
       "4                13           0.925439           0.951542  \n",
       "5                13           0.925439           0.951542  \n",
       "6                25           0.912281           0.955947  \n",
       "7                25           0.912281           0.955947  \n",
       "8                25           0.912281           0.955947  \n",
       "9                 1           0.938596           0.951542  \n",
       "10                1           0.938596           0.951542  \n",
       "11                1           0.938596           0.951542  \n",
       "12               13           0.925439           0.951542  \n",
       "13               13           0.925439           0.951542  \n",
       "14               13           0.925439           0.951542  \n",
       "15               25           0.912281           0.955947  \n",
       "16               25           0.912281           0.955947  \n",
       "17               25           0.912281           0.955947  \n",
       "18                1           0.938596           0.951542  \n",
       "19                1           0.938596           0.951542  \n",
       "20                1           0.938596           0.951542  \n",
       "21               13           0.925439           0.951542  \n",
       "22               13           0.925439           0.951542  \n",
       "23               13           0.925439           0.951542  \n",
       "24               25           0.912281           0.955947  \n",
       "25               25           0.912281           0.955947  \n",
       "26               25           0.912281           0.955947  \n",
       "27                1           0.938596           0.951542  \n",
       "28                1           0.938596           0.951542  \n",
       "29                1           0.938596           0.951542  \n",
       "30               13           0.925439           0.951542  \n",
       "31               13           0.925439           0.951542  \n",
       "32               13           0.925439           0.951542  \n",
       "33               25           0.912281           0.955947  \n",
       "34               25           0.912281           0.955947  \n",
       "35               25           0.912281           0.955947  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "scores_df[['params', 'mean_test_score', 'rank_test_score',\n",
    "'split0_test_score', 'split1_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 정확도: 0.9386\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GridSearchCV를 이용하여 최적으로 학습된 estimator로 \n",
    "# 테스트 데이터 세트에 적용해서 예측 수행. \n",
    "gb_pred = grid_cv.best_estimator_.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "\n",
    "# 결과\n",
    "# 테스트 데이터 세트에서 약 96.06 % 정확도 도출\n",
    "\n",
    "\n",
    "# - GBM은 **`수행 시간이 오래 걸린다`**는 단점이 있지만 **`과적합에도 강해서`** 예측 성능이 뛰어난 알고리즘  \n",
    "# - 많은 알고리즘이 GBM을 기반으로 새롭게 만들어지고 있음\n",
    "#     - 머신러닝 세계에서 가장 각광을 받는 그래디언트 부스팅 기반 ML 패키지\n",
    "#         - XGBoost\n",
    "#         - LightGBM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
